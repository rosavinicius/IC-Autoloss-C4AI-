# IC-Autoloss[C4AI]
Este reposit√≥rio √© dedicado √† documenta√ß√£o do projeto de pesquisa entitulado "AutoLoss: Descobrindo Fun√ß√µes de Perda de Maneira Autom√°tica", orientado pelo Prof. Dr Artur Jord√£o do Departamento de Engenharia de Computa√ß√£o e Sistemas Digitais (PCS) da Poli-usp 

### Sobre o Projeto

Durante a pesquisa, desenvolvemos tr√™s algoritmos inspirados no trabalho da disciplina Aprendizado Profundo - Redes Neurais (PCS5022), por Gustavo Nascimento e Leandro Mugnain. O objetivo principal foi expandir e refinar a busca por fun√ß√µes de perda (loss functions) com maior potencial de desempenho em modelos de aprendizado profundo, com √™nfase em forecasting.

### Gera√ß√£o de Fun√ß√µes de Perda

Criamos um algoritmo de gera√ß√£o pseudo-aleat√≥ria de fun√ß√µes de perda diferenci√°veis, estruturadas como grafos computacionais. Em vez da programa√ß√£o gen√©tica tradicional (baseada em muta√ß√µes e sele√ß√£o por torneio), utilizamos uma abordagem que explora um combina√ß√µes matem√°ticas baseadas em fun√ß√µes primitivas diferenciaveis.
Foram aplicados filtros de rejei√ß√£o para garantir: Exist√™ncia de ≈∑ e y nas express√µes; Converg√™ncia inicial ao treinar um modelo-teste.

### Fus√£o de Modelos com Diferentes Fun√ß√µes de Perda

implementamos um algoritmo para combinar o conhecimento adquirido com diferentes fun√ß√µes de perda. A estrat√©gia consiste em:
	1.	Treinar um modelo de refer√™ncia (ex: com MSE).
	2.	Treinar vers√µes alternativas com outras fun√ß√µes de perda.
	3.	Calcular os deltas de pesos (Œî = w_novo - w_base) de cada fun√ß√£o.
	4.	Somar os deltas e aplic√°-los ao modelo base, criando um modelo final que agrega o aprendizado de todas as fun√ß√µes.

### Combina√ß√£o Otimizada por M√≠nimos Quadrados

Aplicamos o m√©todo dos m√≠nimos quadrados para encontrar a combina√ß√£o √≥tima de fun√ß√µes de perda. A ideia √© ajustar pesos a‚ÇÅ, a‚ÇÇ, ..., a‚Çô que combinem as fun√ß√µes f‚ÇÅ(x), f‚ÇÇ(x), ..., f‚Çô(x) para melhor se aproximarem de uma m√©trica de refer√™ncia q(x) (como MSE ou IOA). Isso permite identificar uma combina√ß√£o mais eficaz do que o uso individual de qualquer fun√ß√£o gerada.

As tabelas abaixo registram o desempenho obtido. √â not√≥rio que, diferentemente da tarefa de classifica√ß√£o, para forecasting, apesar dos avan√ßos em gera√ß√£o autom√°tica de fun√ß√µes de perda, os modelos ainda enfrentam desafios para superar aqueles treinados com fun√ß√µes consagradas. Esse cen√°rio evidencia a oportunidade para investigar novas combina√ß√µes e ajustes finos das fun√ß√µes de perda, de modo a alinhar melhor os crit√©rios de otimiza√ß√£o com as m√©tricas de avalia√ß√£o espec√≠ficas do forecasting, impulsionando, assim, o desempenho e a robustez dos modelos de previs√£o.


| M√©trica | mlp     | rnn                    |
|---------|---------|------------------------|
| mse     | 1.75E-05| 0.00016                |
| 1    | 0.1725  | 0.0470                 |
| 2    | 0.2188  | 0.0498                 |
| 3    | 0.2369  | 0.0505                 |
| 4    | 0.2386  | 0.0535                 |
| 5    | 0.2414  | 0.0548                 |
| fus√£o de pesos   | 0.013   | 0.011   |


as melhores 5 fun√ß√µes encontradas para cada modelo s√£o as seguintes: 
### üìò Fun√ß√µes de Loss ‚Äì MLP

1. ` (≈∑ * (1 / ≈∑ + y |))¬≤`
2. `(log(≈∑))¬≤ * |≈∑ * y|`
3. `exp(sqrt(|≈∑¬≤|)) + 1 / log(sqrt(≈∑ * y))`
4. `(log(≈∑ * y) + exp(tanh(≈∑)))¬≤`
5. `|log(≈∑ + y)|`

---

### üìó Fun√ß√µes de Loss ‚Äì RNN

1. `(≈∑¬≤ * (tanh(≈∑) + ≈∑ * y))¬≤`
2. `(tanh(≈∑ * y) + log(1 / ≈∑))¬≤`
3. `sqrt(|≈∑ + y|)`
4. `1 / |tanh(≈∑ * y)|`
5. `sqrt(tanh((sqrt(≈∑ * y))¬≤))`

Em todos os casos, as restri√ß√µes de dom√≠nio das fun√ß√µes s√£o superadas por meio da introdu√ß√£o de constantes pequenas.


### reprodutibilidade 
Todos os experimentos s√£o pass√≠veis de reprodu√ß√£o. Recomenda-se a cria√ß√£o de um ambiente conforme as diretrizes descritas no arquivo de requisitos (requirements.txt).

